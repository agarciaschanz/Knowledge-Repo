{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students Do: Credit Risk Classification with Amazon SageMaker\n",
    "\n",
    "* **Dataset:** German Credit Risk Dataset - Prof. Dr. Hans Hofmann (original source: [ics.uci.edu](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)), download: [Kaggle](https://www.kaggle.com/uciml/german-credit))\n",
    "\n",
    "* **Goal:** Classify the credit risk of a person as described by a given set of input features.\n",
    "\n",
    "**Note:** You should import and run this notebook into your notebook intance on Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the german_credit_data.csv file (located in ../Resources/) through through JupyterLab\n",
    "file_path = Path(\"Data/german_credit_data.csv\")\n",
    "df = pd.read_csv(file_path, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame with the features (include all columns except \"Risk\")\n",
    "features_df = df.drop(\"Risk\", axis=1)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame with the target data (The \"Risk\" column)\n",
    "target_df = pd.DataFrame(df[\"Risk\"])\n",
    "target_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "A logistic regression model will be trained using all the input features.\n",
    "\n",
    "* `X` is the predictor variable vector with the values of all features.\n",
    "* `Y` is the target variable vector with the risk result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical features (One-hot encode)\n",
    "features_enc = pd.get_dummies(features_df)\n",
    "features_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore DataConversionWarning messages\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(features_enc)\n",
    "scaler\n",
    "print(scaler.mean_[:5])\n",
    "print(scaler.scale_[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_enc_scaled = scaler.transform(features_enc)\n",
    "features_enc_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_enc = pd.get_dummies(target_df)\n",
    "target_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_enc_scaled\n",
    "Y = target_enc[\"Risk_bad\"].values.reshape(-1)  # 0 = Good, 1 = Bad Risk\n",
    "print(X[:5])\n",
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"your_s3_bucket_name_here\"\n",
    "prefix = \"german-credit-risk\"\n",
    "\n",
    "# Amazon SageMaker and related imports\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from sagemaker import get_execution_role\n",
    "import boto3  # AWS Python sdk\n",
    "\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "\n",
    "# AWS IAM role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Training Data to Amazon S3\n",
    "\n",
    "In order to train your machine learning model using Amazon SageMaker, the training data should passed through an Amazon S3 Bucket formatted as a [protobuf recordIO format](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html#td-serialization).\n",
    "\n",
    "The profobuf recordIO format, is a method to serialize structured data (similar to `JSON`), to allow different applications to communicate with each other or for storing data.\n",
    "\n",
    "Using the profobuf recordIO format, allows you to take advantage of _Pipe mode_ when training the algorithms that support it. In _Pipe mode_, your training job streams data directly from Amazon S3. Streaming can provide faster start times for training jobs and better throughput.\n",
    "\n",
    "The following code converts the training data as a Protocol Buffer, next the data is uploaded to the Amazon S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the training data as Protocol Buffer\n",
    "\n",
    "\n",
    "# Upload encoded training data to Amazon S3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload Test Data to Amazon S3\n",
    "\n",
    "If you provide test data, the algorithm logs include the test score for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the testing data as Protocol Buffer\n",
    "\n",
    "\n",
    "# Upload encoded testing data to Amazon S3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Machine Learning Model\n",
    "\n",
    "Once you have uploaded your data to Amazon S3, it's time to train the machine learning model. In this activity, you will use the Amazon SageMaker's [_linear learner algorithm_](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html) to run a linear regression prediction model.\n",
    "\n",
    "You can learn more about the diferent Amazon SageMaker built-in algorithms [in this page](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html).\n",
    "\n",
    "First, an intance of the linear learner algorithm is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the linear learner algorithm\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the estimator container is created in an AWS EC2 instance using a `ml.m4.xlarge`.\n",
    "\n",
    "**Note:** This step might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Amazon SageMaker session\n",
    "sess = \n",
    "\n",
    "# Create an instance of the linear learner estimator\n",
    "linear = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.m4.xlarge\",\n",
    "    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "# Get the dimension of the feature-input vector\n",
    "feature_dim = len(X[:1][0])\n",
    "\n",
    "\n",
    "# Define linear learner hyperparameters\n",
    "# Note how in this case we use: predictor_type='binary_classifier' # (credit risk: good or bad)\n",
    "linear.set_hyperparameters(\n",
    "    feature_dim=feature_dim, mini_batch_size=200, predictor_type=\"binary_classifier\"\n",
    ")\n",
    "\n",
    "# Fitting the linear learner model with the training data\n",
    "linear.fit({\"train\": s3_train_data, \"test\": s3_test_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the Model to Make Predictions\n",
    "\n",
    "In this section, the `linear-learner` model that was trained will be used to make predictions of credit risk. Deploy the model using a `ml.t2.medium` instance type.\n",
    "\n",
    "**Note:** This step might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An instance of the linear-learner predictor is created\n",
    "linear_predictor = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear predictor configurations\n",
    "linear_predictor.content_type = \"text/csv\"\n",
    "linear_predictor.serializer = csv_serializer\n",
    "linear_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making some predictions using the test data.\n",
    "result = linear_predictor.predict(X_test)\n",
    "y_predictions = np.array(\n",
    "    [np.uint8(r[\"predicted_label\"]) for r in result[\"predictions\"]]\n",
    ")\n",
    "y_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "To evaluate the model, we create a confusion matrix to compare the cases where each predicted value matches the expected test-value and when not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Encode the predictios results as 0 = Good, 1 = Bad Risk\n",
    "cat_test = np.array([\"Good\" if x == 0 else \"Bad\" for x in Y_test])\n",
    "cat_pred = np.array([\"Good\" if x == 0 else \"Bad\" for x in y_predictions])\n",
    "\n",
    "# Create the confusion matrix\n",
    "pd.crosstab(cat_test, cat_pred, rownames=[\"actuals\"], colnames=[\"predictions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is also evaluated using the `sklearn` metrics module. The following metrics are calculated:\n",
    "\n",
    "* [`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "\n",
    "* [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "\n",
    "* [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "print(f\"accuracy_score: {accuracy_score(Y_test,y_predictions)}\\n\\n\")\n",
    "print(\n",
    "    f\"sklearn's confusion_matrix: \\n\\n{confusion_matrix(Y_test, y_predictions, labels=[0, 1])}\\n\\n\"\n",
    ")\n",
    "print(\n",
    "    f\"classification_report: \\n\\n{classification_report(Y_test, y_predictions, target_names=['Good', 'Bad'])}\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the end point is deleted to avoid additional AWS resources usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Amazon SageMaker end-point\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
